---
title: "Modele lineaire"
output:
  pdf_document: default
  html_document: default
date: "2024-11-17"
---

## Problématique et présentation des données

Déterminer le risque qu'un sinistre se produisant en ayant à dispositions un jeu de données est au coeur de l'assurance non vie.

Nous étudierons ici le jeu de données "Pluviométrie dans les villes françaises" provenant de <https://husson.github.io/data.html>: c'est un tableau Excel recensant des données météorologiques sur 34 villes françaises en 2023.

Dans le contexte de notre base de données, étudier l'ampleur des précipitations a un intérêt pour par exemple fixer le prix d'un contrat d'assurance habitation.

Notre objectif sera donc de construire un modèle linéaire afin de prédire le niveau des précipitations.

Voici les premières lignes de la base de données :

```{r}
df <- read.table(file = "pluie_france_2023.csv", header = T, sep=";", fileEncoding = "Latin1")
head(df)
```

## Choix du modèle

On va commencer par retirer les variables que l'on sait corrélés:

-   On veut étudier les précipitations annuelles, on retire donc les précipitations mensuelles qui explique entièrement le modèle (colonnes 2 à 13) ainsi que les précipitations sommés de mai à août et de septembre à octobre (colonnes 33 et 34)

-   On retire le nombre de jours de pluie mensuelle pour garder seulement le nombre de jours de pluie annuel (colonnes 15 à 26)

-   La Latitude et la Longitude suffise pour décrire la localisation: on retire le nom des villes (colonne 1) ainsi que "géographie" (colonne 35)

```{r}
df <- df[,-c(1:13,15:26,33:35)]

# On retire les 3 dernières lignes pour tester les différents modèles
data_train <- df[1:(nrow(df) - 3), ] # Valeurs d'entrainement des modèles
data_test <- df[(nrow(df) - 2):nrow(df), ] # Valeurs test des modèles

#On centre et réduits nos données pour faciliter l'analyse et la comparabilités des résultats futurs
data <- as.data.frame(scale(data_train))

#On renomme les variables pour plus de lisibilité plus tard
names(data) <- c('Précipitations', 'Nbr.j.p', 'Temp.moy', 'Amplitude.temp', 'Insolation','Latitude', 'Longitude')

head(data)
```

On fait ensuite une vérification manuelle de la corrélation entre les différentes variables afin d'avoir un bon modèle avant d'entamer le choix de modèle.

```{r}
library(ggplot2)
library(GGally)
ggpairs(data = data,progress=FALSE)
```

Plusieurs problèmes ressortent de ce graphique (dont seules les valeurs de corrélations nous intéressent).

-   1er problème: les variables semblent assez peu corrélés avec les précipitations, rendant difficile sa prédiction (on verra par la suite s'il est préférable de supprimer les variables dont les valeurs de corrélations sont les plus faibles)

-   2nd problème: les variables sembles très corrélés entre elles, ce qui complique l'analyse du modèle et peut fausser les résultats. On a donc envie de retirer la majorité des variables, mais on aura alors un modèle beaucoup trop simpliste pour expliquer le phénomène

Suite à l'observation de ces corrélations, on va poser 3 modèles différents (que l'on ajustera par la suite):

-   **1er modèle:** On supprime toutes les variables fortements corrélés (avec \*\*\* sur le graph) une par une jusqu'à n'avoir que de "faibles" corrélations.

-   **2e modèle:** Pour éviter de supprimer presque toutes les variables on va supprimer seulement celles dont la valeur absolue de corrélation est au dessus de 0.8 .

-   **3e modèle:** On ne supprime aucune variable

## Modèle 1

#### Elimination des variables corrélés

On retire une par une les variables les plus corrélés entre elles (ayant une corrélation noté \*\*\*) jusqu'à n'avoir que des corrélations "mineurs".

Pour savoir quelle variable enlever parmis les deux corrélés, on enlève celle qui est la moins corrélé avec la variable d'intérêt "Précipitations".

Les variables fortement corrélés sont :

1)  "Température moyenne" et "Nombre de jours de pluie" -\> on retire "Température moyenne"

2)  "Insolation" et "Nombre de jours de pluie" -\> on retire "Insolation"

3)  "Insolation" et "Température moyenne" -\> les deux variables sont déjà retirés

4)  "Latitude" et "Nombre de jours de pluie" -\> on retire "Latitude"

5)  "Latitude" et "Température moyenne" -\> les deux variables sont déjà retirés

6)  "Latitude" et "Insolation" -\> les deux variables sont déjà retirés

7)  "Longitude" et "Amplitude des températures" -\> on retire "Longitude"

Finalement nous avons retiré quatre variables ("Température moyenne", "Insolation", "Latitude", "Longitude") et notre modèle est constitués de seulement deux variables explicatives "peu" corrélés: "Nombre de jours de pluie" et "Amplitude des températures".

```{r}
data1 <- data[,-c(3,5,6,7)]
data1
```

Ce nombre de variables est très insuffisant pour un modèle aussi complexe, nous allons donc continuer l'analyse de ce modèle en faisant les techniques habituelles mais on sait déja que le résultat sera complètement faux.

#### Choix du modèle 1

On va déterminer la significativité des deux variables pour le modèle via tout d'abord via une méthode Backward-Forward.

```{r}
mod1 <- lm(Précipitations ~ ., data=data1)
step(mod1,direction="both")
```

On se retrouve avec une seule variable et une régréssion linéaire simple comme étant le modèle optimal. Analysons maintenant la significativé de ce modèle avec une régréssion linéaire.

```{r}
reg.simple <- lm(Précipitations ~ Nbr.j.p, data=data1)
summary(reg.simple)
```

La seule variable restante semble peu significative. Le modèle est donc presque inutile puisqu'on se retrouve avec une valeur du $R^2$ de 0.11; c'est à dire seulement 11% du modèle explique les précipitations.

### Visualisation

Profitons du fait que l'on soit en dimension 1 pour tracer la droite de régréssion:

```{r}
plot1 <- ggplot(data = data1) + aes(x=Précipitations, y=Nbr.j.p) + geom_point()
plot1 <- plot1 + geom_abline(slope = reg.simple$coeff[2], intercept = reg.simple$coeff[1], col = 'red')
plot1 <- plot1 + geom_point( aes(x = reg.simple$model$Nbr.j.p, y = reg.simple$fitted.values), shape = 1, col = "blue", size = 4)

plot1
```

Nous voyons encore une fois que le modèle extrêmement mauvais: retirer toutes les variables corrélés n'est dans ce cas précis pas la meilleure solution, quitte à ce que les données soit un peu biaisés par ces corrélation.

Nous ne ferons pas d'analyses de résidus pour ce 1er modèle, étant donné sa très faible représentation de la variable souhaité les résidus ne serait même pas vraiment des résidus mais simplement des valeurs.

## Modèle 2

#### Elimination des variables corrélés

On applique la même méthode que précédemment, mais seulement pour les variables dont la corrélation en valeur absolue est supérieure à 0.8 :

1)  "Insolation" et "Nombre de jours de pluie" -\> on retire "Insolation"

2)  "Insolation" et "Température moyenne" -\> "Insolation" à déjà été retiré

3)  "Latitude" et "Température moyenne" -\> on retire "Température moyenne"

4)  "Latitude" et "Insolation -\> "Insolation" a déjà été retiré

Finalement nous avons retiré deux variables ("Insolation" et "Température moyenne", ) et notre modèle est constitués de 4 variables explicatives (plus ou moins corrélés): "Nombre de jours de pluie", "Amplitude des températures", "Latitude" et "Longitude".

```{r}
data2 <- data[,-c(5,3)]
data2
```

#### Choix du modèle 2

On utilise à nouveau une méthode Backward-Forwards

```{r}
mod2 <- lm(Précipitations ~ ., data = data2)
step(mod2, direction="both")
```

L'algorithme nous conseil de garder les 4 variables.

Voyons maintenant leur significativité dans le modèle:

```{r}
summary(mod2)
```

Nous remarquons que toutes les variables semblent contenir au moins un peu d'information, avec "Nombre de jours de pluie" et "Latitude" étant très significatifs.

On observe un $R^2$ qui semble correcte, avec une valeur de 59%, soit près et 40% de plus que celui du 1er modèle.

Le modèle semblant plutôt bon, on va faire sur celui-ci une étude des résidus.

Cependant il ne faut pas oublié que ce modèle parait cohérent avec une grande corrélation entre les variables, corrélations dont les effets sur le modèle sont inconnues et seront observés dans les prédictions.

### Etude des résidus

Concentrons-nous sur les résidus studentisés :

```{r}
residus_stu2 <- rstudent(model = mod2)

n <- length(data2$Précipitations)
df.residu_stu2 <- data.frame(residu_stu = residus_stu2)

plot2 <- ggplot(data = df.residu_stu2) + aes(x=1:n, y = residu_stu) + geom_point()
plot2 <- plot2 + geom_hline(yintercept = -2, col = "blue", linetype = 2)
plot2 <- plot2 + geom_hline(yintercept = 2, col = "blue", linetype = 2)
plot2 <- plot2 + xlab('Index') + ylab('Résidus studentisés')
plot2
```

On a environ 11% de résidus en dehors de l'intervalle, ce qui est plus du double des 5% lorsqu'il n'y a pas de valeurs abérrantes, nous sommes donc en présence d'anomalies.

On regarde si ces anomalies ont un grand impact sur le modèle:

```{r}
H <- hatvalues(mod2)
p <- mod2$rank
seuil1 <- 2*p/n
seuil2 <- 3*p/n
df.H <- data.frame(H = H)
ID_levier <- (1:n)[df.H$H>seuil1]
df.H$ID <- rep("",n)
df.H[ID_levier,]$ID <- ID_levier
df.H$group <- rep("Non levier",n)
df.H[ID_levier,]$group <- "Levier"

plot3 <- ggplot(data = df.H) + aes(x=1:n, y = H, color=group) + geom_point()
plot3 <- plot3 + geom_hline(yintercept = seuil1, col = "blue", linetype = 2)
plot3 <- plot3 + geom_hline(yintercept = seuil2, col = "blue", linetype = 3)
plot3 <- plot3 + geom_text(aes(label=ID),hjust=0, vjust=0)
plot3 <- plot3 + xlab('Index') + ylab('Poids des résidus')
plot3
```

Nous observons que deux anomalies sont levier, dont une fortement. Nous regardons la distance de Cook pour avoir une idée de cette influence.

```{r}
cook <- cooks.distance(mod2)
df.cook <- data.frame(cook = cook)
s1 <- qf(0.5,p,n-p)
s2 <- qf(0.1,p,n-p)
plot4 <- ggplot(data = df.cook) + aes(x=1:n, y = cook) + geom_point()
plot4 <- plot4 + geom_hline(yintercept = s1, col = "blue", linetype = 2)
plot4 <- plot4 + geom_hline(yintercept = s2, col = "blue", linetype = 3)
plot4 <- plot4 + xlab('Index') + ylab('Distance de Cook')
plot4
```

Concernant la distance de Cook nous observons qu'une seule valeur est vraiment abérrante: la 1ere. Or cette valeur correspond à celle de la ville d'Ajaccio.

Cette abération pourrait donc s'expliquer par le climat différent en Corse qu'en France métropolitaine ainsi que son éloignement géographique par rapport aux autres villes, on va donc décider d'exclure cette variable du modèle (ainsi que des modèles à venir).

```{r}
data_v2 <- data[-c(1),]
data2_v2 <- data2[-c(1),]

mod2_v2 <- lm(Précipitations ~ ., data = data2_v2)
residus_stu2_v2 <- rstudent(model = mod2_v2)
p_v2 <- mod2_v2$rank

n_2 <- length(data2_v2$Précipitations)
df.residu_stu2_v2 <- data.frame(residu_stu = residus_stu2_v2)
```

Observons maintenant la répartition des résidus pour conclure sur leur normalité.

```{r}
quant.t <- qt((1:n_2)/n_2,n_2-p-1)
df_qq <- data.frame(Obs = sort(df.residu_stu2_v2$residu_stu), Theo = quant.t)
qq.plot <- ggplot(data = df_qq, aes(x = Obs, y = Theo)) + geom_point(shape = 1, size = 2.5)
qq.plot <- qq.plot + geom_abline(slope = 1, intercept = 0,col = "blue", linetype = 2, size = 0.5)
qq.plot <- qq.plot + xlab("Quantiles empiriques des résidus") + ylab("Student T(n-p-1)")
qq.plot <- qq.plot + xlim(-3,4) + ylim(-3,4)
qq.plot
```

```{r}
ks.test(x = df.residu_stu2$residu_stu, y = 'pt', df = n-p-1)
```

On peut conclure d'après le test de Kolmogorov-Smirnov et les quantiles empiriques des résidus studentisés que les résidus suivent une loi gausienne centré réduite

### Prédiction du modèle

```{r}
data_test1 <- as.data.frame(scale(data_test)) #Le data_test est centré réduit

predit1 <- coef(mod2)[1] - 1.1406469*coef(mod2)[2] - 0.05123155*coef(mod2)[3] - 1.1053143*coef(mod2)[4] - 0.2619903*coef(mod2)[5] + rnorm(1,0,1) 
#Prédiction sur 1ere ligne du data_test
erreur1 <- abs(predit1 - -0.8403231)

predit2 <- coef(mod2)[1] + 0.4147807*coef(mod2)[2] - 0.97339949*coef(mod2)[3] + 0.8419845*coef(mod2)[4] - 0.8429252*coef(mod2)[5] + rnorm(1,0,1) 
#Prédiction sur la 2e ligne du data_test
erreur2 <- abs(predit2 - -0.2656904)

predit3 <- coef(mod2)[1] + 0.7258662*coef(mod2)[2] + 1.02463104*coef(mod2)[3] + 0.2633298*coef(mod2)[4] + 1.1049155*coef(mod2)[5] + rnorm(1,0,1) 
#Prédiction sur la 3e ligne du data_test
erreur3 <- abs(predit3 - 1.1060135)

print(paste("Le pourcentage d'erreur pour la 1ere ligne est", abs(erreur1/-0.8403231)*100, "%"))
print(paste("Le pourcentage d'erreur pour la 2e ligne est", abs(erreur2/-0.2656904)*100, "%"))
print(paste("Le pourcentage d'erreur pour la 3e ligne est", abs(erreur3/1.1060135)*100, "%"))
```

## Modèle 3

#### Choix du modèle 3

Puisque dans ce modèle on conserve toutes les variables corrélés, on peut directement faire la méthode du Backward-Forward:

```{r}
mod3 <- lm(Précipitations~.,data=data_v2)
step(mod3,direction="both")
```

L'algorithme nous conseil de garder les variables "Nombre de jour de pluie", "Insolation", "Amplitude des températures", "Latitude" et "Longitude" et de supprimer "Température moyenne".

On a donc a 5 variables pour expliquer notre modèle, plus grand nombre jusqu'à présent.

Regréssion linéaire:

```{r}
data3 <- data_v2[,-c(3)]
mod4 <- lm(Précipitations~.,data=data3)
summary(mod4)
```

Nous remarquons que toutes les variables semblent très significative, avec de loin le meilleur $R^2$ observé jusqu'à maintenant, d'une valeur de 82% (même en prenant le $R^2_a$ qui pénalise le nombre de variables on est toujours à 20% de plus que le R² du modèle 2).

Nous rappelons encore une fois que ce modèle paraît certe le plus représentatif mais est aussi le plus corrélé.

#### Etude des résidus

Concentrons-nous encore une fois sur les résidus studentisés :

```{r}
residus_stu3 <- rstudent(model = mod4)

n <- length(data3$Précipitations)
df.residu_stu3 <- data.frame(residu_stu = residus_stu3)

plot2 <- ggplot(data = df.residu_stu3) + aes(x=1:n, y = residu_stu) + geom_point()
plot2 <- plot2 + geom_hline(yintercept = -2, col = "blue", linetype = 2)
plot2 <- plot2 + geom_hline(yintercept = 2, col = "blue", linetype = 2)
plot2 <- plot2 + xlab('Index') + ylab('Résidus studentisés')
plot2
```

On observe 6% de valeurs en dehors de l'intervalle, ce qui est proche de la limite de 5% et il paraît donc acceptable de dire qu'il n'y a pas de données abérrantes.

Etant donnée la faible fiabilité du modèle nous allons tout de même vérifier cela.

On regarde si des anomalies ont un grand impact sur le modèle:

```{r}
H <- hatvalues(mod4)
p <- mod4$rank
seuil1 <- 2*p/n
seuil2 <- 3*p/n
df.H <- data.frame(H = H)
ID_levier <- (1:n)[df.H$H>seuil1]
df.H$ID <- rep("",n)
df.H[ID_levier,]$ID <- ID_levier
df.H$group <- rep("Non levier",n)
df.H[ID_levier,]$group <- "Levier"

plot3 <- ggplot(data = df.H) + aes(x=1:n, y = H, color=group) + geom_point()
plot3 <- plot3 + geom_hline(yintercept = seuil1, col = "blue", linetype = 2)
plot3 <- plot3 + geom_hline(yintercept = seuil2, col = "blue", linetype = 3)
plot3 <- plot3 + geom_text(aes(label=ID),hjust=0, vjust=0)
plot3 <- plot3 + xlab('Index') + ylab('Poids des résidus')
plot3
```

Nous observons 2 points de levier, tous relativement faiblement.

Nous regarderons donc la distance de Cook pour avoir une idée des influences de ces leviers.

```{r}
cook <- cooks.distance(mod4)
df.cook <- data.frame(cook = cook)
s1 <- qf(0.5,p,n-p)
s2 <- qf(0.1,p,n-p)
plot4 <- ggplot(data = df.cook) + aes(x=1:n, y = cook) + geom_point()
plot4 <- plot4 + geom_hline(yintercept = s1, col = "blue", linetype = 2)
plot4 <- plot4 + geom_hline(yintercept = s2, col = "blue", linetype = 3)
plot4 <- plot4 + xlab('Index') + ylab('Distance de Cook')
plot4
```

Un seul point dépasse le 1er seuil, et aucun des points ne dépasse le seuil problématique, on peut donc garder le modèle tel quel.

Observons maintenant la répartition des résidus pour conclure sur leur normalité.

```{r}
quant.t <- qt((1:n)/n,n-p-1)
df_qq <- data.frame(Obs = sort(df.residu_stu3$residu_stu), Theo = quant.t)

qq.plot <- ggplot(data = df_qq, aes(x = Obs, y = Theo)) + geom_point(shape = 1, size = 2.5)
qq.plot <- qq.plot + geom_abline(slope = 1,intercept = 0, col = "blue", linetype = 2, size = 0.5)
qq.plot <- qq.plot + xlab("Quantiles empiriques des résidus") + ylab("Student T(n-p-1)")
qq.plot <- qq.plot + xlim(-3,4) + ylim(-3,4)
qq.plot
```

```{r}
ks.test(x = df.residu_stu3$residu_stu, y = 'pt', df = n-p-1)
```

On peut conclure d'après le test de Kolmogorov-Smirnov et les quantiles empiriques des résidus studentisés que les résidus suivent une loi gausienne.

#### Prédiction du modèle

```{r}
predit1 <- coef(mod4)[1] - 1.1406469*coef(mod4)[2] - 0.05123155*coef(mod4)[3] - 1.1053143*coef(mod4)[4] - 0.2619903*coef(mod4)[5] + rnorm(1,0,1)
#Prédiction sur 1ere ligne du data_test
erreur1 <- abs(predit1 - -0.8403231)

predit2 <- coef(mod4)[1] + 0.4147807*coef(mod4)[2] - 0.97339949*coef(mod4)[3] + 0.8419845*coef(mod4)[4] - 0.8429252*coef(mod4)[5] + rnorm(1,0,1)
#Prédiction sur 2e ligne du data_test
erreur2 <- abs(predit2 - -0.2656904)

predit3 <- coef(mod4)[1] + 0.7258662*coef(mod4)[2] + 1.02463104*coef(mod4)[3] + 0.2633298*coef(mod4)[4] + 1.1049155*coef(mod4)[5] + rnorm(1,0,1)
#Prédiction sur 3e ligne du data_test
erreur3 <- abs(predit3 - 1.1060135)

print(paste("Le pourcentage d'erreur pour la 1ere ligne est", abs(erreur1/-0.8403231)*100, "%"))
print(paste("Le pourcentage d'erreur pour la 2e ligne est", abs(erreur2/-0.2656904)*100, "%"))
print(paste("Le pourcentage d'erreur pour la 3e ligne est", abs(erreur3/1.1060135)*100, "%"))
```

Et on retrouve encore une fois cette volatilité, mais présente de manière encore plus forte.

# Comparaison des prédictions

On remarque que malgré un $R^2$ bien meilleur pour le 3e modèle, les predictions de celui-ci sont encore plus mauvaises que celles du second modèle. Nous pouvons associer cela à une conséquence de la plus grande corrélation des variables dans le dernier modèle.

Parmis les trois modèles proposés, même si aucun n'est satisfaisant, on privilegiera quand même l'utilisation du second, qui est moins volatile dans ses résulats et qui à de manière général un pourcentage d'erreur plus faible sur les prédictions.

# Conclusion: problèmes de la base choisie

Nous avons vu à travers ce projet que toutes les modélisations était mauvaises, mais alors comment expliquer ces résulats ? Plusieurs choses sont a mentionnés:

-   Presque toutes les variables sont très corrélés entre elles, rendant très compliqué l'analyse ou la faussant complètement

-   On a un nombre très faible d'échantillon: 34 dont 3 utilisés comme tests et 1 retiré car non compatible (Ajaccio) et donc au final seulement 30 entrainant le modèle

    -   Chaque échantillon représente des villes très éloignés les unes des autres

    -   On a un unique échantillon par ville (impossible de ce concentrer sur une seule, ce qui ferait un bien meilleur modèle)

-   Cette analyse est innaproprié pour ce sujet, nous avons mal choisis notre base:

    -   Du fait de la diversité des climats en France (montagne, littoral,...) il est impossible de séparer les variables latitude et longitude de sorte que chacune donne des informations indépendamment de l'autre : la Bretagne et l'Alsace ont environ la même latitude et pourtant elles ont un climat très différent car la longitude est opposé, or cela ne peut pas s'exprimer dans un modèle linéaire

    -   Le même problème serait apparu si on avait privilégié les variables qualitatives "Nord, Sud, Est, Ouest", c'est encore une fois beaucoup trop large.

    -   On a $Y=X\beta$ et ici notre $X$ n'a donc ici pas de sens puisque les valeurs d'estimations de $\beta$ ne s'affinent pas avec l'augmentation du nombre de $X_i$ : au contraire chaque information donnée par un $X_i$ rentre en conflit (dans l'estimation des $\beta$ ) avec les informations données par les autres $X_i$ .

Même avec un jeu de donnée avec beaucoup plus de ville et avec des variables non corrélés cela ne changerait pas le fait que le sujet est mal choisi : les variables $X_i$ se contredisent.
